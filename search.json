[
  {
    "objectID": "posts/Thesis_work/index.html",
    "href": "posts/Thesis_work/index.html",
    "title": "Making LLaMa-2 answer the Bar.",
    "section": "",
    "text": "Law and AI (made with deepai.org)\n\n\nThe following blog post talks about my escapades with my master thesis, titled “Supervised Fine-Tuning of Open LLMs for Law: Training and Evaluation for Performance in Legal Examinations”. # Motivation - To begin with, the main motivation of the thesis was to bring together somehow an open source ( atleast open weight model) and the legal domain. The field of law is steeped in complex terminologies, rich contextual dependencies and a heavy basis in the applications of concepts learned over years of study into real life ( where alot of scenarios are very case specific), it seemed like a good testing bed for the premise of using a large language model for the task # Existing literature - I did an extensive literature survey, which is provided in my thesis report for those interested in reading further. I will condense it here and also expand a few things in the interest of building a cohesive story here :) ## Encoder based models - a Lot of legal LLM applications, at the time of my literature survey, worked on applying *Encoder based models to specific legal tasks. These tasks ranged from niche functions such as contract-review, Legal rule classification, court judgement prediction and other legal stuff that i cannot recall exactly but remember were beyond my comprehension at the time ( probably still ). - One thing that bugged me, was that these studies and datasets involved alot of preprocessing and informed legal knowledge ( as such many of the authors of these works are lawyers, and rightly so), that it seemed that the adoption of encoder based models was not a straight forward task. - The question that begged to be answered, was that, given the pervasive rise of llms, and their adoption into so many different purpose that already signified their extreme potential for generalising to tasks with simple, accessible and well established fine-tuning methods ( thanks majorly in part to huggingface libraries, atleast in my project); could LLMs be fine-tuned with a fraction of the effort to perform well in the legal domain? ## Existing shows of performance of LLMs in Legal domains ### Decoder-Based Models: What About Their Role in Law? - This is where things get interesting. Decoder-only models like GPT-3 and GPT-4 have certainly made headlines with some impressive achievements. Take GPT-4, for instance: it managed to pass the bar exam, hitting an impressive 90th percentile (although they reported the best performing instance, so tiny bit of cherry picking there). They showed that, without extra legal training, a model Like GPT4 could reason through legal questions with some accuracy. But here’s the catch—it wasn’t fine-tuned specifically for legal tasks. They tested it with bar questions as-is, highlighting that while these models can handle broad legal tasks, they’re not yet positioned to replace human legal advisors. - This led me to wonder: could decoder-only models like GPT take on more specialized legal tasks without that level of setup? While some studies (like DISC-LawLLM) show that decoder models can handle open-ended legal tasks, they still aren’t the first choice for specialized legal functions, where encoder models remain dominant. ### Are Decoder Models Ready for Legal Use-cases? - Decoder models show promise for generalized legal Q&A systems, particularly with recent advances in customization tools (thanks to Hugging Face, which i have used extensively for the pipelines i implemented). What makes them so exciting is that you dont necessarily need to teach the already vastly knowlegdable model a new corpus of text, rather you can just fine-tune the model to teach it the necessary style adaptations and get (hopefully)promising results. \nDummy new page"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog cholo"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Reans Blog",
    "section": "",
    "text": "Making LLaMa-2 answer the Bar.\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2024\n\n\n\n\n\n\nNo matching items"
  }
]